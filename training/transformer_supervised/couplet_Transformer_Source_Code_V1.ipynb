{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hululuzhu/chinese-ai-writing-share/blob/main/training/transformer_supervised/couplet_Transformer_Source_Code_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQAh-D0FCzwV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Chinese Couplet Transformer model source code. e.g.\n",
    "\n",
    "```\n",
    "上: 欢天喜地度佳节\n",
    "下: 举国迎春贺新年\n",
    "上: 不待鸣钟已汗颜，重来试手竟何艰\n",
    "下: 只缘沧海常风雨，再去翻身只等闲\n",
    "上: 相思俱付三更月\n",
    "下: 寂寞难留一夜风\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntLVC-_5FaH3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Connect to google drive and prepare local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4tEmatSFNTf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive') # mount to google drive to save models after training\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['TF_KERAS'] = '1'\n",
    "!pip install keras-transformer &> /dev/null\n",
    "from keras_transformer import get_model, decode, get_custom_objects\n",
    "\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbZJpPPdhH21",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## TPU setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXx7Yz25hH22",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "# print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "strategy = tf.distribute.TPUStrategy(resolver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9kOwaYYfAAG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Fetch Data and extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjU1zyfWFj4f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "working_dir = \"/tmp/working_dir\"\n",
    "!mkdir -p {working_dir}\n",
    "!wget https://github.com/wb14123/couplet-dataset/releases/download/1.0/couplet.tar.gz -P {working_dir}\n",
    "!ls -l {working_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSq9OXajj1hG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p {working_dir}/couplet_files\n",
    "!tar -xf {working_dir}/couplet.tar.gz -C {working_dir}/couplet_files\n",
    "# !ls -l -R  /tmp/working_dir/couplet_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptcOb8M-Kq2B",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!head -1 {working_dir}/couplet_files/couplet/train/in.txt {working_dir}/couplet_files/couplet/train/out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peOzerlpke4B",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Get vocabs of all chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvLAnInGj74m",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "COUPLET_PATH = f'{working_dir}/couplet_files/couplet'\n",
    "token_dict = {\n",
    "    '<PAD>': 0,\n",
    "    '<START>': 1,\n",
    "    '<END>': 2,\n",
    "}\n",
    "with open(f\"{COUPLET_PATH}/vocabs\", \"r\") as f:\n",
    "  for x in f:\n",
    "    c = x.strip()[0]\n",
    "    if c not in token_dict:\n",
    "      token_dict[c] = len(token_dict)\n",
    "\n",
    "for t in ['train', 'test']:\n",
    "  for i in ['in', 'out']:\n",
    "    with open(f\"{COUPLET_PATH}/{t}/{i}.txt\", \"r\") as f:\n",
    "      for line in f:\n",
    "        for cs in line.strip().replace(' ', '').replace('\\n', ''):\n",
    "          for c in cs:\n",
    "            if c not in token_dict:\n",
    "              token_dict[c] = len(token_dict)\n",
    "\n",
    "assert 9132 == len(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YsfGuIet71lg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join('/content/gdrive/MyDrive/ML/Models/szhu_public_062021', 'couplet_vocab.pickle'), 'wb') as handle:\n",
    "    pickle.dump(token_dict, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDttIXl-l88h",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rev_token_dict = {v: k for k, v in token_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGHOCOZTLUDK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Encode data (chars to char-ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7t-2Ly6LWTB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 34  # 32 chars plus start/end\n",
    "\n",
    "def clean_input(rawq):\n",
    "  return rawq.strip().replace(' ', '')\n",
    "\n",
    "def encode(rawq, is_decode_output = False, is_2d=False):\n",
    "  output = []\n",
    "  if not is_decode_output:\n",
    "    output.append([1] if is_2d else 1) # start added to encode/decode outputs\n",
    "  # content encoding\n",
    "  string_leng = len(rawq.strip().replace(' ', ''))\n",
    "  for c in rawq.strip().replace(' ', ''):\n",
    "    if c not in token_dict:\n",
    "      token_dict[c] = len(token_dict)\n",
    "    output.append([token_dict[c]] if is_2d else token_dict[c])\n",
    "  output.append([2] if is_2d else 2) # end\n",
    "  for i in range(MAX_SEQ_LEN - len(output)):\n",
    "    output.append([0] if is_2d else 0) # padding to fixed MAX_SEQ_LEN size\n",
    "  return output\n",
    "\n",
    "train_raw = {\"in\": [], \"out\": [], \"pre\": [], \"post\": [], \"decode_in\": []}\n",
    "test_raw = {\"in\": [], \"out\": [], \"pre\": [], \"post\": [], \"decode_in\": []}\n",
    "total_raw = {'train': train_raw, 'test': test_raw}\n",
    "\n",
    "for t in ['train', 'test']:\n",
    "  for i in ['in', 'out']:\n",
    "    with open(f\"{COUPLET_PATH}/{t}/{i}.txt\", \"r\") as f:\n",
    "      for line in f:\n",
    "        if i == 'out':\n",
    "          total_raw[t]['decode_in'].append(encode(line, False, i=='in'))\n",
    "        total_raw[t][i].append(encode(line, i=='out', i=='out'))\n",
    "        total_raw[t][\"pre\" if i == 'in' else 'post'].append(clean_input(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEK47o_BOTkB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def decode_tokens(token_ids):\n",
    "  output = \"\"\n",
    "  for token_id in token_ids:\n",
    "    if token_id > 2:\n",
    "      output += rev_token_dict[token_id]\n",
    "    elif token_id == 0:\n",
    "      break\n",
    "  return output\n",
    "\n",
    "for inq, indecode, outq in zip(total_raw['train']['in'][:3],\n",
    "                     total_raw['train']['decode_in'][:3],\n",
    "                     total_raw['train']['out'][:3]):\n",
    "  print(inq, \"\\n\", indecode, \"\\n\", outq)\n",
    "  print(decode_tokens(inq), decode_tokens(np.asarray(outq).reshape(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8ynitPfQLsT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "\n",
    "for t in ['train', 'test']:\n",
    "  dfs[t] = pd.DataFrame(\n",
    "      list(zip(total_raw[t]['in'], total_raw[t]['out'], total_raw[t]['pre'], total_raw[t]['post'], total_raw[t]['decode_in'])),\n",
    "      columns =['in', 'out', 'pre', 'post', 'decode_in'])\n",
    "  dfs[t]['in_length']  = dfs[t]['in'].str.len()\n",
    "  dfs[t]['out_length']  = dfs[t]['out'].str.len()\n",
    "  dfs[t]['de_in_length']  = dfs[t]['decode_in'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63PXWgKzSXKO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dfs['train'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keFfK9OWuNoM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Transformer model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0465hKlj5vkE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "in_np = np.array(dfs['train']['in'].values.tolist())\n",
    "decode_in_np = np.array(dfs['train']['decode_in'].values.tolist())\n",
    "out_np = np.asarray(dfs['train']['out'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6sMwRFtU-aA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "in_np_test = np.array(dfs['test']['in'].values.tolist())\n",
    "decode_in_np_test = np.array(dfs['test']['decode_in'].values.tolist())\n",
    "out_np_test = np.array(dfs['test']['out'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYAaGohtXcSa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(in_np.shape, decode_in_np.shape, out_np.shape)\n",
    "print(in_np_test.shape, decode_in_np_test.shape, out_np_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0C4eYwYx_aYl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  num_encoders = 4\n",
    "  num_docoders = 4\n",
    "  num_heads = 8\n",
    "  embed_size = 64 * num_docoders\n",
    "  drop_out_rate = 0.1\n",
    "  model = get_model(\n",
    "    token_num=len(token_dict),\n",
    "    embed_dim=embed_size,\n",
    "    encoder_num=num_encoders,\n",
    "    decoder_num=num_docoders,\n",
    "    head_num=num_heads,\n",
    "    hidden_dim=embed_size,\n",
    "    attention_activation='gelu',\n",
    "    feed_forward_activation='gelu',\n",
    "    dropout_rate=drop_out_rate,\n",
    "    embed_weights=np.random.random((len(token_dict), embed_size)),\n",
    "  )\n",
    "  model.compile(\n",
    "      optimizer=tf.keras.optimizers.Adam(),\n",
    "      loss='sparse_categorical_crossentropy',\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ytBCHjck6yCq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 80\n",
    "batch_size = 256\n",
    "model.fit(\n",
    "  x=[in_np, decode_in_np],\n",
    "  y=out_np,\n",
    "  batch_size=batch_size,\n",
    "  epochs=epochs,\n",
    "  validation_data=([in_np_test, decode_in_np_test], out_np_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDeGJzTKhH3C",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-q9YMYshH3C",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DRIVE_MODEL_DIR = '/content/gdrive/MyDrive/ML/Models/chinese_couplet_v1'\n",
    "!mkdir -p {DRIVE_MODEL_DIR}\n",
    "model.save_weights(DRIVE_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qk0h37bTovKU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Inference, see [this colab](https://github.com/hululuzhu/chinese-ai-writing-share/blob/main/RC_01_AI_Writing_Demo_06_2021.ipynb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "tpu_couplet_writing_0816",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}