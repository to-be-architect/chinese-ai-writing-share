{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hululuzhu/chinese-ai-writing-share/blob/main/training/transformer_supervised/poem_Transformer_Source_Code_Share_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opLDHSMPBx5w",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Colab to train a Chinese poem writing transformer. e.g.\n",
    "\n",
    "```\n",
    "标题: 秋思\n",
    "正文: 秋风吹雨过，秋色满江城。一叶无人到，千山有客情。\n",
    "标题: 百度\n",
    "正文: 百尺孤城上，千金万里中。山川无限水，水石有余风。\n",
    "标题: 湾区春日之谜\n",
    "正文: 春风吹雨不成秋，春色如何一日休。不是春光无处着，只应春色是人愁。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlJ3e7rtOSGp",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Xqtm9Zw_OTby",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "!pip install -q \"tqdm>=4.36.1\" > /tmp/na\n",
    "from tqdm.notebook import tqdm\n",
    "!pip install chinese-converter > /tmp/na\n",
    "import chinese_converter\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip install keras-transformer &> /dev/null\n",
    "os.environ['TF_KERAS'] = '1'\n",
    "from keras_transformer import get_model, decode, get_custom_objects\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rx33fOJthmlz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LrOV92qahml0",
    "outputId": "fa57349b-b013-40a8-bb33-7ff16820caf5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.115.204.170:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.115.204.170:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "# print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "strategy = tf.distribute.TPUStrategy(resolver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "av1LR9NcssbX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Connect to Google Drive for storage\n",
    "- useful to store model or dict/params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDHpUVGasvE7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Mount your google drive if you haven't\n",
    "!mkdir -p drive/MyDrive/ML/Models/chinese_poem_v1\n",
    "WORK_DIR = 'drive/MyDrive/ML/Models/chinese_poem_v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mmgXuGWs5M2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load data and transform and persist to Drive (no need to rerun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ai47AGm0kQ7t",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/chinese-poetry/chinese-poetry\n",
    "POEM_CONTENT = {\n",
    "    'tang': {\n",
    "        'total': 58,\n",
    "        'pattern': \"https://raw.githubusercontent.com/chinese-poetry/chinese-poetry/master/json/poet.tang.{0}.json\"\n",
    "    },\n",
    "    'song': {\n",
    "        'total': 255,\n",
    "        'pattern': \"https://raw.githubusercontent.com/chinese-poetry/chinese-poetry/master/json/poet.song.{0}.json\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_poems(is_test=True, verbose=True):\n",
    "  df_list = []\n",
    "  for dynasty in POEM_CONTENT:\n",
    "    size = 3 if is_test else POEM_CONTENT[dynasty]['total']\n",
    "    pbar = tqdm(total=size, desc=\"Dynasty \" + dynasty)\n",
    "    for i in range(size):\n",
    "      url = POEM_CONTENT[dynasty]['pattern'].format(i * 1000)\n",
    "      if verbose:\n",
    "        print(f\"download {url} now\")\n",
    "      df_list.append(pd.read_json(url))\n",
    "      pbar.update(1)\n",
    "  return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2neo-AtkTfB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = get_poems(is_test=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uxa5z5QVkV_g",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['concat_paragraphs'] = [''.join(map(str, l)) for l in df['paragraphs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuDXzadwnMND",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = df[['author', 'title', 'concat_paragraphs']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LH4suWB3oZFA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Convert to simplified Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-T17tTioi_Q",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convert_schinese(tchinese):\n",
    "  return chinese_converter.to_simplified(tchinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pD5v3ILoobP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['s_content'] = df.apply(lambda row: convert_schinese(''.join(row.concat_paragraphs)), axis=1)\n",
    "df['s_title'] = df.apply(lambda row: convert_schinese(''.join(row.title)), axis=1)\n",
    "df['s_author'] = df.apply(lambda row: convert_schinese(''.join(row.author)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6NP9Qcu0OjA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_df = df[['s_content', 's_title', 's_author']]\n",
    "my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KrQ7NtkXyOTZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for key in my_df.columns:\n",
    "  print(my_df[key][:].apply(len).describe())\n",
    "\n",
    "def trim_author_fn(row):\n",
    "  return row.s_author[:4]\n",
    "\n",
    "def trim_title_fn(row):\n",
    "  trimed_title = row.s_title[:12].replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "  return trimed_title\n",
    "\n",
    "def trim_content_fn(row):\n",
    "  trimed_content = row.s_content[:64]\n",
    "  last_period = trimed_content.rfind(\"。\")\n",
    "  return trimed_content[:last_period+1]\n",
    "\n",
    "# Trim the size\n",
    "my_df['s_author'] = my_df.apply(trim_author_fn, axis=1)\n",
    "my_df['s_title'] = my_df.apply(trim_title_fn, axis=1)\n",
    "my_df['s_content'] = my_df.apply(trim_content_fn, axis=1)\n",
    "\n",
    "\n",
    "# TODO, find space in title and choose 1st part\n",
    "# TODO, find last period of content and stop there after triming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K02qD15vvhsj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "short_mask = (my_df['s_title'].str.len() == 0) | (my_df['s_content'].str.len() <= 10) | ('无正文' == my_df['s_content']) | ('无正文' == my_df['s_author'])\n",
    "filter_my_df = my_df.loc[~short_mask]\n",
    "filter_my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5mBqixf7Oie",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filter_my_df[filter_my_df['s_content'].str.len() <= 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVvEZpiopbqQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Get Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A847PRQUpcyc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "token_dict = {\n",
    "  '<PAD>': 0,\n",
    "  '<START>': 1,\n",
    "  '<END>': 2,\n",
    "}\n",
    "\n",
    "def process_token(token_dict, df):\n",
    "  for field in df.columns:\n",
    "    for title in df[field]:\n",
    "      for c in title:\n",
    "        if c not in token_dict:\n",
    "          token_dict[c] = len(token_dict)\n",
    "\n",
    "process_token(token_dict, filter_my_df)\n",
    "rev_token_dict = {v: k for k, v in token_dict.items()}\n",
    "vocab_size = len(token_dict)\n",
    "\n",
    "print(\"vocab_size\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5y6Vdq3C-piK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Persist DF and Dictionary for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yLcSBBn-s0D",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(WORK_DIR, 'vocab_0604_v1.pickle'), 'wb') as handle:\n",
    "    pickle.dump(token_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "filter_my_df.to_pickle(os.path.join(WORK_DIR, 'dataframe_300k_0604_v1.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q8lCv9FA_Wo9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!ls -l {WORK_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuL7sHzvANuD",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train model from Title to Content (without author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzP1TBto_jUB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Reload from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUZYJjAM_rQD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loaded_token_dict = pickle.load(\n",
    "    open(os.path.join(WORK_DIR, 'vocab_0604_v1.pickle'), \"rb\" ))\n",
    "\n",
    "loaded_df = pd.read_pickle(\n",
    "    os.path.join(WORK_DIR, 'dataframe_300k_0604_v1.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9h9Km6zu_l5q",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rev_token_dict = {v: k for k, v in loaded_token_dict.items()}\n",
    "\n",
    "assert 11289 == len(rev_token_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQS0KSuNO7dC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZAW3g-ZNEn9a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MAX_INPUT_SEQ = 14 # max title length + 2 special tokens\n",
    "MAX_OUTPUT_SEQ = 66 # max 64 content length + 2 special tokens\n",
    "START_TOKEN_ID = loaded_token_dict['<START>']\n",
    "END_TOKEN_ID = loaded_token_dict['<END>']\n",
    "PAD_TOKEN_ID = loaded_token_dict['<PAD>']\n",
    "\n",
    "\n",
    "def encode(raw_text, is_decode_input, is_decode_output):\n",
    "  assert not (is_decode_input and is_decode_output)\n",
    "  output = []\n",
    "  if not is_decode_output:\n",
    "    output.append(START_TOKEN_ID)\n",
    "  for c in raw_text:\n",
    "    output.append(loaded_token_dict[c])\n",
    "  output.append(END_TOKEN_ID)\n",
    "  # padding\n",
    "  total_size = MAX_OUTPUT_SEQ if is_decode_input or is_decode_output else MAX_INPUT_SEQ\n",
    "  for i in range(total_size - len(output)):\n",
    "    output.append(PAD_TOKEN_ID)\n",
    "  return output\n",
    "\n",
    "def decode(token_ids):\n",
    "  output = \"\"\n",
    "  for token_id in token_ids:\n",
    "    if token_id > 2:\n",
    "      output += rev_token_dict[token_id]\n",
    "    elif token_id == 0:\n",
    "      break\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8z07rA2VF0-k",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(encode('登竺云山', is_decode_input = False, is_decode_output = False))\n",
    "print(encode('独上千峰与万峰，晴岚淡写海江容', is_decode_input = True, is_decode_output = False))\n",
    "print(encode('独上千峰与万峰，晴岚淡写海江容', is_decode_input = False, is_decode_output = True))\n",
    "\n",
    "print(decode([1, 546, 4787, 35, 344, 2, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
    "print(decode([1, 302, 167, 17, 168, 481, 185, 168, 8, 773, 2281, 3939, 94, 342, 1566, 1563, 2, 0, 0,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WRXAws88JwoV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# Shuffle the order of df\n",
    "# TEST_RATE = 0.03\n",
    "shuffle_loaded_df = loaded_df.sample(frac=1).reset_index(drop=True)\n",
    "cutoff = 6000 # Use 6k as test\n",
    "df_test = shuffle_loaded_df[:cutoff]\n",
    "df_train = shuffle_loaded_df[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gSGMIKe7KJk_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_ds(df):\n",
    "  text_x = df['s_title'].values\n",
    "  text_y = df['s_content'].values\n",
    "  x = np.asarray([encode(k, False, False) for k in text_x])\n",
    "  x_d = np.asarray([encode(k, True, False) for k in text_y])\n",
    "  # final output need extra 1 dim\n",
    "  y = np.expand_dims(np.asarray([encode(k, False, True) for k in text_y]), -1)\n",
    "  return x, x_d, y\n",
    "\n",
    "train_x, train_x_d, train_y = prepare_ds(df_train)\n",
    "test_x, test_x_d, test_y = prepare_ds(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ylQu67xM0U6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(train_x.shape, train_x_d.shape, train_y.shape)\n",
    "print(test_x.shape, test_x_d.shape, test_y.shape)\n",
    "\n",
    "print(decode(train_x[1000]), decode(train_x_d[1000]), decode(np.squeeze(train_y[1000], -1)))\n",
    "print(decode(test_x[1000]), decode(test_x_d[1000]), decode(np.squeeze(test_y[1000], -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALWUKqUsPofT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Build transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVne6yNePqjc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  num_encoders = 4\n",
    "  num_docoders = 4\n",
    "  num_heads = 8\n",
    "  embed_size = 64 * num_docoders\n",
    "  drop_out_rate = 0.3\n",
    "  model = get_model(\n",
    "    token_num=len(rev_token_dict),\n",
    "    embed_dim=embed_size,\n",
    "    encoder_num=num_encoders,\n",
    "    decoder_num=num_docoders,\n",
    "    head_num=num_heads,\n",
    "    hidden_dim=embed_size,\n",
    "    attention_activation='gelu',\n",
    "    feed_forward_activation='gelu',\n",
    "    dropout_rate=drop_out_rate,\n",
    "    embed_weights=np.random.random((len(rev_token_dict), embed_size)),\n",
    "  )\n",
    "  model.compile(\n",
    "      optimizer=tf.keras.optimizers.Adam(),\n",
    "      loss='sparse_categorical_crossentropy',\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QE3KmHYUQX5G",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 60  # 60 is minimal to be meaningful\n",
    "batch_size = 128\n",
    "model.fit(\n",
    "  x=[train_x, train_x_d],\n",
    "  y=train_y,\n",
    "  batch_size=batch_size,\n",
    "  epochs=epochs,\n",
    "  validation_data=([test_x, test_x_d], test_y),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bn9DZER_hml_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Save your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EveqcNBchml_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p {WORK_DIR}/model_weights\n",
    "model.save_weights(f'{WORK_DIR}/model_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6bU4cYq6e66",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Inference, please see [this colab](https://github.com/hululuzhu/chinese-ai-writing-share/blob/main/RC_01_AI_Writing_Demo_06_2021.ipynb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "中文写诗Transformer Source Code Share V1",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}